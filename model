from typing import Optional, List
import torch
import torch.nn as nn
import torch.nn.functional as F
from timm.models.layers import DropPath


class DualCrossAttentionModule(nn.Module):
	"""
	Module to fuse two input sequences via cross-attention and feed-forward networks.

	Parameters:
		in_features (int): Dimensionality of input features.
		cross_heads (int): Number of attention heads for cross-attention.
		num_layers (int): Number of cross-attention layers.
		dropout (float): Dropout probability.
		drop_path (float): Maximum drop path rate.
		seq_length (int): Length of the input sequences.

	Input:
		input1: Tensor of shape (B, seq_length, in_features)
		input2: Tensor of shape (B, seq_length, in_features)

	Output:
		Fused tensor of shape (B, seq_length, in_features)
	"""
	
	def __init__(self, in_features: int, cross_heads: int, num_layers: int,
	             dropout: float, drop_path: float, seq_length: int):
		super().__init__()
		self.num_layers = num_layers
		
		# Positional encodings for both inputs
		self.position_encoding1 = nn.Parameter(torch.randn(1, seq_length, in_features) * 0.02)
		self.position_encoding2 = nn.Parameter(torch.randn(1, seq_length, in_features) * 0.02)
		
		# Build cross-attention layers
		self.attention_layers = nn.ModuleList([
			nn.MultiheadAttention(embed_dim=in_features,
			                      num_heads=cross_heads,
			                      dropout=dropout,
			                      batch_first=True)
			for _ in range(num_layers)
		])
		
		# Build feed-forward network layers
		self.feedforward_layers = nn.ModuleList([
			nn.Sequential(
				nn.Linear(in_features, in_features * 4),
				nn.GELU(),
				nn.Linear(in_features * 4, in_features)
			)
			for _ in range(num_layers)
		])
		
		# Normalization for attention and feed-forward outputs
		self.norm_attn = nn.ModuleList([nn.LayerNorm(in_features) for _ in range(num_layers)])
		self.norm_ffn = nn.ModuleList([nn.LayerNorm(in_features) for _ in range(num_layers)])
		
		# Stochastic depth (DropPath) layers
		drop_probs = torch.linspace(0, drop_path, steps=num_layers).tolist()
		self.drop_path_layers = nn.ModuleList([
			DropPath(p) if p > 0.0 else nn.Identity() for p in drop_probs
		])
		self.dropout = nn.Dropout(dropout)
	
	def forward(self, input1: torch.Tensor, input2: torch.Tensor) -> torch.Tensor:
		# Add positional encodings
		query = input1 + self.position_encoding1
		context = input2 + self.position_encoding2
		
		for i in range(self.num_layers):
			# Cross-attention: use 'context' as key and value
			attn_output, _ = self.attention_layers[i](query=query, key=context, value=context)
			query = self.norm_attn[i](query + self.drop_path_layers[i](attn_output))
			# Feed-forward network with residual connection
			ffn_output = self.feedforward_layers[i](query)
			query = self.norm_ffn[i](query + self.drop_path_layers[i](ffn_output))
		return query


class ClsTransformerEncoder(nn.Module):
	"""
	Transformer encoder with a dedicated CLS token; returns the CLS token representation.

	Parameters:
		embed_dim (int): Embedding dimension.
		attn_heads (int): Number of attention heads.
		num_layers (int): Number of transformer layers.
		dropout (float): Dropout probability.
		seq_length (int): Length of the input sequence.

	Input:
		x: Tensor of shape (B, seq_length, embed_dim)

	Output:
		Tensor of shape (B, embed_dim) representing the CLS token.
	"""
	
	def __init__(self, embed_dim: int, attn_heads: int, num_layers: int,
	             dropout: float, seq_length: int):
		super().__init__()
		self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)
		self.pos_embed = nn.Parameter(torch.randn(1, seq_length + 1, embed_dim) * 0.02)
		
		encoder_layers = [
			nn.TransformerEncoderLayer(
				d_model=embed_dim,
				nhead=attn_heads,
				dim_feedforward=embed_dim * 4,
				dropout=dropout,
				activation='gelu',
				batch_first=True
			)
			for _ in range(num_layers)
		]
		self.encoder = nn.Sequential(*encoder_layers)
		self.layer_norm = nn.LayerNorm(embed_dim)
	
	def forward(self, x: torch.Tensor) -> torch.Tensor:
		batch_size = x.size(0)
		cls_tokens = self.cls_token.expand(batch_size, -1, -1)
		# Prepend the CLS token
		x = torch.cat([cls_tokens, x], dim=1)
		x = x + self.pos_embed
		encoded = self.encoder(x)
		return self.layer_norm(encoded[:, 0])


class MultiPerspectiveEmbedder(nn.Module):
	"""
	Generates multi-perspective embeddings from input features using three linear projections
	and simple weighted fusion.

	Parameters:
		input_dim (int): Dimension of the input features.
		out_dim (int): Output dimension per view.
		num_views (int): Number of views.
		dropout (float): Dropout probability.

	Input:
		x: Tensor of shape (B, input_dim)

	Output:
		Tensor of shape (B, num_views, out_dim)
	"""
	
	def __init__(self, input_dim: int, out_dim: int, num_views: int, dropout: float = 0.1):
		super().__init__()
		self.num_views = num_views
		self.out_dim = out_dim
		
		# Three linear projections for different views
		self.proj_view1 = nn.Linear(input_dim, out_dim)
		self.proj_view2 = nn.Linear(input_dim, out_dim * num_views)
		self.proj_view3 = nn.Linear(input_dim, out_dim)
		
		self.activation = nn.ReLU()
		self.dropout = nn.Dropout(dropout)
		self.layer_norm = nn.LayerNorm(out_dim * num_views)
		
		# Learnable fusion weights for the three projections
		self.fusion_weights = nn.Parameter(torch.ones(3))
	
	def forward(self, x: torch.Tensor) -> torch.Tensor:
		batch_size = x.size(0)
		# Compute three view representations
		view1 = self.proj_view1(x)  # (B, out_dim)
		view2 = self.activation(self.proj_view2(x))  # (B, out_dim * num_views)
		view3 = self.proj_view3(x)  # (B, out_dim)
		
		# Reshape view2 and expand view3 to match num_views
		view2 = view2.view(batch_size, self.num_views, self.out_dim)  # (B, num_views, out_dim)
		view3 = view3.unsqueeze(1).expand(-1, self.num_views, -1)  # (B, num_views, out_dim)
		
		# Weighted fusion of the three views
		fused = (view1.unsqueeze(1) * self.fusion_weights[0] +
		         view2 * self.fusion_weights[1] +
		         view3 * self.fusion_weights[2])
		
		# Apply dropout and layer normalization
		fused = fused.view(batch_size, -1)
		fused = self.dropout(fused)
		fused = self.layer_norm(fused)
		fused = fused.view(batch_size, self.num_views, self.out_dim)
		return fused


class AMFT(nn.Module):
	"""
	Adaptive Multiview Fusion Transformer (AMFT)

	This is the main model which integrates dual-modal fusion, transformer encoding, and classification.
	The model is designed for adaptive fusion of multi-view features and is suitable for multimodal tasks.

	Parameters:
		trans_dim (int): Dimension for transformer layers.
		feature_dim (int): Dimension of raw EEG features.
		n_classes (int): Number of output classes.
		dropout (float): Dropout probability.
		drop_path (float): Maximum drop path rate.
		fusion_heads (int): Number of attention heads in the fusion module.
		fusion_layers (int): Number of layers in the fusion module.
		trans_heads (int): Number of attention heads in the main transformer.
		trans_layers (int): Number of transformer layers.
		seq_length (int): Sequence length.

	Abbreviation:
		AMFT
	"""
	
	def __init__(self,
	             trans_dim: int = 512,
	             feature_dim: int = 310,
	             n_classes: int = 3,
	             dropout: float = 0.1,
	             drop_path: float = 0.4,
	             fusion_heads: int = 4,
	             fusion_layers: int = 2,
	             trans_heads: int = 4,
	             trans_layers: int = 1,
	             seq_length: int = 5):
		super().__init__()
		self.feature_dim = feature_dim
		self.n_classes = n_classes
		
		# Dual cross-attention fusion module
		self.fusion_module = DualCrossAttentionModule(
			in_features=trans_dim,
			cross_heads=fusion_heads,
			num_layers=fusion_layers,
			dropout=dropout,
			drop_path=drop_path,
			seq_length=seq_length
		)
		
		# Transformer encoder with a CLS token
		self.cls_encoder = ClsTransformerEncoder(
			embed_dim=trans_dim,
			attn_heads=trans_heads,
			num_layers=trans_layers,
			dropout=dropout,
			seq_length=seq_length
		)
		
		# Final classification layer
		self.classifier = nn.Linear(trans_dim, n_classes)
		
		# Multi-perspective embedder for EEG features
		self.eeg_embedder = MultiPerspectiveEmbedder(
			input_dim=feature_dim,
			out_dim=trans_dim,
			num_views=seq_length
		)
		
		self._initialize_weights()
	
	def _initialize_weights(self):
		"""
		Initialize weights using Xavier uniform initialization for Linear and MultiheadAttention layers,
		and constant initialization for LayerNorm biases and weights.
		"""
		for module in self.modules():
			if isinstance(module, nn.Linear):
				nn.init.xavier_uniform_(module.weight)
				if module.bias is not None:
					nn.init.constant_(module.bias, 0)
			elif isinstance(module, nn.LayerNorm):
				nn.init.constant_(module.weight, 1.0)
				nn.init.constant_(module.bias, 0)
			elif isinstance(module, nn.MultiheadAttention):
				if hasattr(module, 'in_proj_weight') and module.in_proj_weight is not None:
					nn.init.xavier_uniform_(module.in_proj_weight)
				if hasattr(module, 'out_proj'):
					nn.init.xavier_uniform_(module.out_proj.weight)
					if module.out_proj.bias is not None:
						nn.init.constant_(module.out_proj.bias, 0)
	
	def extract_features(self, eeg_inputs: List[torch.Tensor], aux: Optional[torch.Tensor] = None) -> torch.Tensor:
		"""
		Extract features by fusing two EEG inputs.

		Args:
			eeg_inputs (List[Tensor]): List containing two EEG tensors, each of shape (B, seq_length, feature_dim).
			aux (Tensor, optional): Additional features (unused).

		Returns:
			Tensor: Feature representation of shape (B, trans_dim)
		"""
		batch_size = eeg_inputs[0].size(0)
		# Flatten EEG inputs to shape (B, -1)
		eeg1 = eeg_inputs[0].reshape(batch_size, -1)
		eeg2 = eeg_inputs[1].reshape(batch_size, -1)
		
		embed1 = self.eeg_embedder(eeg1)
		embed2 = self.eeg_embedder(eeg2)
		
		fused_output = self.fusion_module(embed1, embed2)
		features = self.cls_encoder(fused_output)
		return features
	
	def forward(self,
	            eeg: Optional[List[torch.Tensor]] = None,
	            aux: Optional[torch.Tensor] = None,
	            alpha: float = 1.0,
	            reverse: bool = False) -> torch.Tensor:
		"""
		Forward pass for classification.

		Args:
			eeg (List[Tensor], optional): EEG inputs [view1, view2] with shape (B, seq_length, feature_dim).
			aux (Tensor, optional): Additional features (unused).
			alpha (float): Gradient reversal coefficient (unused).
			reverse (bool): Flag for gradient reversal (unused).

		Returns:
			Tensor: Logits of shape (B, n_classes)
		"""
		if eeg is None:
			raise ValueError("EEG inputs must be provided.")
		features = self.extract_features(eeg, aux)
		logits = self.classifier(features)
		return logits
